{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This challenge will help you gain intuition on how a **K-means** clustering works.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that our data actually contains relevant clusters, we will generate it ourselves.\n",
    "\n",
    "To do so, we will use [`make_blobs`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) from `sklearn`.  \n",
    "\n",
    "We want a dataset with **500 observations**, **2 features** and **4 clusters**.  \n",
    "\n",
    "We use *random_state=42* so that you can compare results with your buddy.\n",
    "\n",
    "ğŸ‘‡ Run the cell below to generate your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=42\n",
    "\n",
    "# Generate data\n",
    "X, y = make_blobs(n_samples=500, centers=4, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ Make a scatter plot of your two features against each other  \n",
    "â“ Color the points according to their corresponding  value in `y`\n",
    "\n",
    "<details span=\"markdown\">\n",
    "    <summary>ğŸ’¡ Help</summary>\n",
    "\n",
    "- Recall the color argument:\n",
    " - `c` for matplotlib  \n",
    " - `hue` for seaborn \n",
    "\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘† You should see 4 distinct clusters, each with a different color.  \n",
    "\n",
    "Training a KNN Classifier on `X` with `y` as target would give great results.\n",
    "\n",
    "However today is about **Unsupervised Learning**\n",
    "\n",
    "Let's assume that we never knew about `y` and only received `X` to work with.  \n",
    "\n",
    "We only have 2 features and no target ğŸ˜±  \n",
    "\n",
    "**What can we can achieve with only `X` ?**  ğŸ¤”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Apply K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal is to find the **number of clusters** that best matches the structure of your data.  \n",
    "\n",
    "\n",
    "ğŸ‘‰Import `KMeans` from `sklearn` and initiate a model with the parameters:\n",
    "- `n_clusters=2`,\n",
    "- `random_state=42`\n",
    "\n",
    "â“Fit the model on your `X`  \n",
    "â“ Get your predictions and store them in a `y_pred` variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘† The predictions are a vector of cluster assignment for each observation.  \n",
    "With `n_clusters=2` each observation in `X` will be associated to either one of two clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ Make a scatter plot of your two features against each other  \n",
    "â“ Color the points according the predicted cluster in `y_pred`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘† You can still see 4 distinct clusters, however the color only show 2. ğŸ˜±\n",
    "\n",
    "That's exactly what we asked for when giving `n_clusters=2` to our **Kmeans**.\n",
    "\n",
    "We asked our algorithm to assign each observation to one of two centroids, hence one of two clusters only.\n",
    "\n",
    "**This clustering around 2 centers is clearly no optimal, we can do better** ğŸ’ª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§ª Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('two_means', clusters=y_pred)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Find the optimal number of clusters  \n",
    "*The Elbow Method*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once fitted, the `KMeans` instance gains an attribute named `inertia_`.\n",
    "\n",
    "It represents the **sum of squared distances of observations to their associated (closest) cluster center**. \n",
    "\n",
    "So the lower, the better.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans(n_clusters=2, random_state=random_state).fit(X).inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘† Think of this in comparison to the Sum of Squared Errors in a Linear Regression ğŸ‘‡  \n",
    "\n",
    "- `SSE` of a `Linear Regression` ğŸ‘‰ `Sum of squared distances between observations and the regression line`  \n",
    "\n",
    "- `Inertia` of a `KMeans Clustering` ğŸ‘‰ `Sum of squared distances between observations and their closest centroid`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way for us to find the optimal number of clusters is a heuristic: the **Elbow Method**.  \n",
    "\n",
    "We have to try several number of clusters and look at the inertia obtained for each one.  \n",
    "\n",
    "Let's do it to get the intuition! âš™ï¸ğŸ§ \n",
    "\n",
    "â“ Fit a `KMeans` for every number of clusters between 1 and 10, for each one, save the inertia in a list `wcss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the elbow method to find the optimal number of clusters.\n",
    "wcss = []\n",
    "clusters = list(range(1, 11))\n",
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“  Plot the inertias in `wcss` against their corresponding number of clusters â“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘† We clearly see an Elbow at 4 clusters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) K-Means with optimal clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the optimal number of clusters know, it's time to fit a last `KMeans`.\n",
    "\n",
    "â“ Fit a `KMeans` with `n_clusters=4` on your `X`, store the predictions in `y_pred`  \n",
    "\n",
    "â“ Make a scatter plot of your two features against each other, and color the points  according the predicted cluster in `y_pred`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is clustering at its best \tğŸ†  \n",
    "We successfully identified **4 clusters** among our observations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** Scaling features before clustering is not always necessary, but it rarely hurts ğŸ˜‡  \n",
    "You can check these [detailed answers](https://datascience.stackexchange.com/questions/6715/is-it-necessary-to-standardize-your-data-before-clustering) to go further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ **Don't forget to push your notebook.**  \n",
    "\n",
    "Proceed the challenges of the day and come back here if you have time ğŸ˜‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Optional) Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An other method that will help us find the optimal number of clusters is called [Hierarchical Clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering).\n",
    "\n",
    "ğŸ‘‰ Plot the **dendrogram** of the hierarchical clustering.\n",
    "\n",
    "You will need the help of two scipy classes found in its [cluster.hierarchy](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html) module:\n",
    "- [linkage](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage)\n",
    "- [dendrogram](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html#scipy.cluster.hierarchy.dendrogram)\n",
    "\n",
    "The linkage actually does the hierarchical clustering with a bottom-up approach: each observation starts in its own cluster.  \n",
    "\n",
    "At each iteration the algorithm will choose clusters to merge, the last iteration occurs when a single cluster is formed.  \n",
    "\n",
    "Here we will use Ward's method, which minimizes the total within-cluster variance.  \n",
    "So at each iteration the algorithm will find the pair of clusters which merging will minimize the within-cluster variance. \n",
    "\n",
    "A dendrogram is just a diagram representing a tree-like structure, it allows us to visualize the linkage.  \n",
    "\n",
    "<br>\n",
    "<details span=\"markdown\">\n",
    "    <summary>ğŸ’¡ Solution</summary>\n",
    "\n",
    "```python\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "Z = linkage(X, method='ward')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "dendrogram(Z, color_threshold=50)\n",
    "plt.title('Dendrogram')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.xticks([]);\n",
    "```\n",
    "---\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still see 4 distinct clusters  \n",
    "\n",
    "ğŸ¤” Why did we used the *Ward method* here?\n",
    "\n",
    "ğŸ’¡ Because it is the same objective function than K-means, which tries to minimize the inertia (i.e. the within cluster variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights on Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‘‰ Watch this video to better understand hierarchical clustering: https://www.youtube.com/watch?v=7xHsRkOdVwo\n",
    "\n",
    "ğŸ‘‰ Hierarchical clustering is considered *greedy*, meaning that it is likely to yield a local optima and miss the big picture\n",
    "\n",
    "ğŸ‘‰ Note that there exist other clustering linkages (ie. other ways to measure distance between two clusters) <img src=https://editor.analyticsvidhya.com/uploads/40351linkages.PNG>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
